{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5683b8e7-0d05-46fd-876b-57a5d89c983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from time import perf_counter\n",
    "from dataclasses import dataclass\n",
    "from itertools import product\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "DTYPE = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52f104f-62ce-4685-b1b6-9434f45a7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Counter:\n",
    "    def __init__(self):\n",
    "        self.reads, self.writes = 0, 0\n",
    "    def inc_reads(self, n): self.reads += n    # increment reads\n",
    "    def inc_writes(self, n): self.writes += n  # increment writes\n",
    "    def show(self): print(f\"Total reads: {self.reads}\\nTotal writes: {self.writes}\")\n",
    "\n",
    "def timeit(func: Callable):\n",
    "    def timer(*args, **kwargs):\n",
    "        t1 = perf_counter()\n",
    "        out = func(*args, **kwargs)\n",
    "        t2 = perf_counter()\n",
    "        diff = (t2 - t1) * 1000 # in ms\n",
    "        print(f\"Time elapsed: {diff:.2f} ms\")\n",
    "        return out\n",
    "    return timer\n",
    "\n",
    "def cdiv(a: int, b: int): return (a + b - 1) // b # equivalent to math.ceil()\n",
    "\n",
    "@dataclass\n",
    "class D3:\n",
    "    z: int = 1\n",
    "    y: int = 1\n",
    "    x: int = 1\n",
    "    @property\n",
    "    def size(self): return self.x * self.y * self.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecaff1e0-0059-4b01-ad3b-cac7ddccbed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_cuda_kernel(blocks: D3, threads: D3, malloc_shared_size: int=0, kernel: Callable=None):\n",
    "    assert kernel is not None\n",
    "    @timeit\n",
    "    def dispatch_kernel(*kernargs):\n",
    "        for bz, by, bx in product(range(blocks.z), range(blocks.y), range(blocks.x)):\n",
    "            shared_mem = torch.empty((malloc_shared_size,), dtype=DTYPE)\n",
    "            for tz, ty, tx in product(range(threads.z), range(threads.y), range(threads.x)):\n",
    "                kernel(D3(bz, by, bx), D3(tz, ty, tx), threads, shared_mem, *kernargs)\n",
    "    return dispatch_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddce8301-973a-478e-8c38-f54540e571b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.03,  2.09, -9.03,  2.60,  3.83, -6.59,  7.71, -4.47, -6.40,  3.55,  0.10,  2.37, -3.79,  1.86, -2.75],\n",
      "        [ 9.02, -0.43, -6.69, -2.15,  2.83, -4.01, -0.50,  4.68, -0.97,  6.35, -3.11,  0.69, -8.55,  1.70, -3.31],\n",
      "        [-3.66, -8.28,  8.09, -3.47,  0.45, -5.76,  4.54, -0.28, -8.23, -9.75,  1.97,  4.39, -0.50,  0.77, -0.22],\n",
      "        [-2.60,  4.26, -0.18, 13.37, -5.49, 13.48,  4.82, -0.98,  3.19,  6.84, 10.74, -7.64,  7.85, -2.85, -7.24],\n",
      "        [-5.90,  5.01,  3.96,  0.84, -0.10, -4.95,  6.50,  5.18,  4.60,  4.21,  3.62, -8.26,  8.03,  3.57, -5.75],\n",
      "        [ 2.15,  0.55, -0.10, -2.93,  1.21,  4.64, -1.13,  6.23, -1.13,  0.17,  8.36, -1.05, -5.91,  2.76, -1.31],\n",
      "        [-1.76, -2.08, -0.72, 10.56, -0.92,  5.09, 10.48, -4.15,  2.66,  6.33,  3.80, -6.08,  5.25, -1.04, -8.29],\n",
      "        [-1.68,  6.65, -2.83, 12.43, -5.49, 13.44, -0.77,  0.32,  4.95, 11.65,  8.31, -8.69,  7.76,  2.99, -8.59],\n",
      "        [-3.49, -5.51, -2.55, -2.55, -4.06,  4.61, -0.28, -7.77,  4.09,  7.14,  0.77, -3.77,  6.35,  5.99, -0.95],\n",
      "        [ 0.29, -5.40,  5.98, -5.65, -3.91,  1.73, -2.49,  5.38,  6.87,  1.03, -4.10,  3.05,  5.24,  0.24, -0.83],\n",
      "        [ 1.63,  3.26,  5.76,  2.44,  0.86,  3.54, -3.64,  3.53,  5.75, -1.05,  8.15,  0.28,  0.36, -1.06,  5.23],\n",
      "        [-1.72,  0.96,  1.68, -0.48,  3.09,  2.22, -0.79,  1.06,  1.20,  7.95, -0.33, -3.27, -3.12,  8.30, -7.55]])\n"
     ]
    }
   ],
   "source": [
    "# test example\n",
    "M, K, N = 12, 24, 15\n",
    "A = torch.randn(M, K, dtype=DTYPE)\n",
    "B = torch.randn(K, N, dtype=DTYPE)\n",
    "C_ref = A @ B\n",
    "print(C_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb7521e-5f28-4f9f-b013-e5bf5faa1821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(C: Tensor): return \"PASSED\" if torch.allclose(C, C_ref, atol=1e-6) else \"FAILED\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede18360-02b4-4ec2-b71d-3c3cf3314708",
   "metadata": {},
   "source": [
    "# Matmul Naive 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec45f063-215b-4712-b4fa-c2a03d40e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_naive_kernel(blockIdx, threadIdx, blockSize, shared_mem,\n",
    "                        C: Tensor, A: Tensor, B: Tensor, M: int, K: int, N: int, counter: Counter):\n",
    "    idx = (blockIdx.x * blockSize.x) + threadIdx.x\n",
    "    if idx < M * N:\n",
    "        acc = 0.\n",
    "        m, n = idx // N, idx % N\n",
    "        for k in range(K):\n",
    "            acc += A[m * K + k] * B[k * N + n]\n",
    "            counter.inc_reads(2)\n",
    "        C[idx] = acc\n",
    "        counter.inc_writes(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64999740-393e-42e2-9f25-dd43845724f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_naive(A: Tensor, B: Tensor):\n",
    "    (M, K), (K_, N) = A.shape, B.shape\n",
    "    assert K == K_, f\"inner dims should match! {K} != {K_}\"\n",
    "    A = A.contiguous() if not A.is_contiguous() else A\n",
    "    B = B.contiguous() if not B.is_contiguous() else B\n",
    "    C = torch.empty(M, N, dtype=DTYPE)\n",
    "\n",
    "    threads = 8\n",
    "    blocks = cdiv(M * N, threads)\n",
    "    kernel = launch_cuda_kernel(D3(x=blocks), D3(x=threads), kernel=matmul_naive_kernel)\n",
    "    kernel(C.flatten(), A.flatten(), B.flatten(), M, K, N, counter:=Counter())\n",
    "    counter.show()\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12fdd2c7-3693-4bae-b4c6-8b08b56162b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 18.41 ms\n",
      "Total reads: 8640\n",
      "Total writes: 180\n",
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "C = matmul_naive(A, B)\n",
    "print(check(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8989d087-f875-4ac7-a48a-21076ce0b1c6",
   "metadata": {},
   "source": [
    "# Matmul Naive 2D Tiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dc1b533-64ab-4555-9c8c-0b643ab12114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_naive2D_kernel(blockIdx, threadIdx, blockSize, shared_mem,\n",
    "                          C: Tensor, A: Tensor, B: Tensor, M: int, K: int, N: int, tileWidth: int, counter: Counter):\n",
    "    idx_m = (blockIdx.y * blockSize.y) + threadIdx.y\n",
    "    idx_n = (blockIdx.x * blockSize.x) + threadIdx.x\n",
    "    tiles_k = cdiv(K, tileWidth)\n",
    "    if idx_m < M and idx_n < N:\n",
    "        for tile_k in range(tiles_k):\n",
    "            for k in range(tileWidth):\n",
    "                idx_k = tile_k * tileWidth + k\n",
    "                if idx_k < K:\n",
    "                    idx_a = idx_m * K + idx_k\n",
    "                    idx_b = idx_k * N + idx_n\n",
    "                    idx_c = idx_m * N + idx_n\n",
    "                    C[idx_c] += A[idx_a] * B[idx_b]\n",
    "                    counter.inc_reads(2)\n",
    "                    counter.inc_writes(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5d020a3-dbd3-456d-8d53-48e067426a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_naive2D(A: Tensor, B: Tensor, tileWidth: int):\n",
    "    (M, K), (K_, N) = A.shape, B.shape\n",
    "    assert K == K_, f\"inner dims should match! {K} != {K_}\"\n",
    "    A = A.contiguous() if not A.is_contiguous() else A\n",
    "    B = B.contiguous() if not B.is_contiguous() else B\n",
    "    C = torch.empty(M, N, dtype=DTYPE)\n",
    "\n",
    "    blocks_m = cdiv(M, tileWidth)\n",
    "    blocks_n = cdiv(N, tileWidth)\n",
    "    kernel = launch_cuda_kernel(\n",
    "        D3(y=blocks_m, x=blocks_n), D3(y=tileWidth, x=tileWidth), kernel=matmul_naive2D_kernel)\n",
    "    kernel(C.flatten(), A.flatten(), B.flatten(), M, K, N, tileWidth, counter:=Counter())\n",
    "    counter.show()\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81165d62-5374-4a4d-8f7a-4a23193aed58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 29.50 ms\n",
      "Total reads: 8640\n",
      "Total writes: 4320\n",
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "C = matmul_naive2D(A, B, 5)\n",
    "print(check(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad847a-b3c6-4bc8-879a-89ed5a59713f",
   "metadata": {},
   "source": [
    "# Matmul 2D Tiled (Shared Memory Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c95319fe-3423-4cfe-b4f3-786e397291d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_2Dshared_kernel(blockIdx, threadIdx, blockSize, shared_mem,\n",
    "                           C: Tensor, A: Tensor, B: Tensor, M: int, K: int, N: int, tileWidth: int, counter: Counter):\n",
    "    idx_m = (blockIdx.y * blockSize.y) + threadIdx.y\n",
    "    idx_n = (blockIdx.x * blockSize.x) + threadIdx.x\n",
    "    shared_offset = tileWidth ** 2\n",
    "    tiles_k = cdiv(K, tileWidth)\n",
    "    if idx_m < M and idx_n < N:\n",
    "        for tile_k in range(tiles_k):\n",
    "            # load tile onto shared memory\n",
    "            for k in range(tileWidth):\n",
    "                idx_k = tile_k * tileWidth + k\n",
    "                idx_a = idx_m * K + idx_k\n",
    "                idx_b = idx_k * N + idx_n\n",
    "                idx_shared_a = threadIdx.y * tileWidth + k\n",
    "                idx_shared_b = k * tileWidth + threadIdx.x + shared_offset\n",
    "                shared_mem[idx_shared_a] = A[idx_a] if idx_k < K else 0\n",
    "                shared_mem[idx_shared_b] = B[idx_b] if idx_k < K else 0\n",
    "                if idx_k < K: counter.inc_reads(2)\n",
    "\n",
    "            # compute dot products\n",
    "            for k in range(tileWidth):\n",
    "                idx_k = tile_k * tileWidth + k\n",
    "                if idx_k < K:\n",
    "                    idx_shared_a = threadIdx.y * tileWidth + k\n",
    "                    idx_shared_b = k * tileWidth + threadIdx.x + shared_offset\n",
    "                    idx_c = idx_m * N + idx_n\n",
    "                    C[idx_c] += shared_mem[idx_shared_a] * shared_mem[idx_shared_b]\n",
    "                    counter.inc_writes(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "117991f0-fdf4-432c-b3da-8f060063a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_2Dshared(A: Tensor, B: Tensor, tileWidth: int):\n",
    "    (M, K), (K_, N) = A.shape, B.shape\n",
    "    assert K == K_, f\"inner dims should match! {K} != {K_}\"\n",
    "    A = A.contiguous() if not A.is_contiguous() else A\n",
    "    B = B.contiguous() if not B.is_contiguous() else B\n",
    "    C = torch.empty(M, N, dtype=DTYPE)\n",
    "\n",
    "    blocks_m = cdiv(M, tileWidth)\n",
    "    blocks_n = cdiv(N, tileWidth)\n",
    "    kernel = launch_cuda_kernel(\n",
    "        D3(y=blocks_m, x=blocks_n), D3(y=tileWidth, x=tileWidth), 2*tileWidth**2, kernel=matmul_2Dshared_kernel)\n",
    "    kernel(C.flatten(), A.flatten(), B.flatten(), M, K, N, tileWidth, counter:=Counter())\n",
    "    counter.show()\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09f30530-4b18-4360-b6c8-52b81ff21c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 53.01 ms\n",
      "Total reads: 8640\n",
      "Total writes: 4320\n",
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "C = matmul_2Dshared(A, B, 3)\n",
    "print(check(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a935917-0870-4c1f-a2d3-e9b43e73b907",
   "metadata": {},
   "source": [
    "# Matmul Naive 3D Tiled (for fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b763202-7a5a-49ac-a827-1849792770fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_naive3D_kernel(blockIdx, threadIdx, blockSize, shared_mem,\n",
    "                          C: Tensor, A: Tensor, B: Tensor, M: int, K: int, N: int, counter: Counter):\n",
    "    idx_m = (blockIdx.z * blockSize.z) + threadIdx.z\n",
    "    idx_n = (blockIdx.y * blockSize.y) + threadIdx.y\n",
    "    idx_k = (blockIdx.x * blockSize.x) + threadIdx.x\n",
    "    if idx_m < M and idx_n < N and idx_k < K:\n",
    "        idx_a = idx_m * K + idx_k\n",
    "        idx_b = idx_k * N + idx_n\n",
    "        idx_c = idx_m * N + idx_n\n",
    "        C[idx_c] += A[idx_a] * B[idx_b]\n",
    "        counter.inc_reads(2)\n",
    "        counter.inc_writes(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21c9a76e-eebf-4e43-83bf-99902c2c6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_naive3D(A: Tensor, B: Tensor, tileWidth: int):\n",
    "    (M, K), (K_, N) = A.shape, B.shape\n",
    "    assert K == K_, f\"inner dims should match! {K} != {K_}\"\n",
    "    A = A.contiguous() if not A.is_contiguous() else A\n",
    "    B = B.contiguous() if not B.is_contiguous() else B\n",
    "    C = torch.empty(M, N, dtype=DTYPE)\n",
    "\n",
    "    blocks_m = cdiv(M, tileWidth)\n",
    "    blocks_n = cdiv(N, tileWidth)\n",
    "    blocks_k = cdiv(K, tileWidth)\n",
    "    kernel = launch_cuda_kernel(\n",
    "        D3(z=blocks_m, y=blocks_n, x=blocks_k), D3(tileWidth, tileWidth, tileWidth), kernel=matmul_naive3D_kernel)\n",
    "    kernel(C.flatten(), A.flatten(), B.flatten(), M, K, N, counter:=Counter())\n",
    "    counter.show()\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f914e7a-f68f-4122-979b-041320508b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 34.81 ms\n",
      "Total reads: 8640\n",
      "Total writes: 4320\n",
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "C = matmul_naive3D(A, B, 3)\n",
    "print(check(C))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
