{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5683b8e7-0d05-46fd-876b-57a5d89c983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from time import perf_counter\n",
    "from dataclasses import dataclass\n",
    "from itertools import product\n",
    "from threading import Barrier\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd893cd-06a1-4e9c-9d39-258eb5752965",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "DTYPE = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52f104f-62ce-4685-b1b6-9434f45a7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Counter:\n",
    "    def __init__(self):\n",
    "        self.reads, self.writes = 0, 0\n",
    "    def inc_reads(self, n): self.reads += n    # increment reads\n",
    "    def inc_writes(self, n): self.writes += n  # increment writes\n",
    "    def show(self): print(f\"Total reads: {self.reads}\\nTotal writes: {self.writes}\")\n",
    "\n",
    "def timeit(func: Callable):\n",
    "    def timer(*args, **kwargs):\n",
    "        t1 = perf_counter()\n",
    "        out = func(*args, **kwargs)\n",
    "        t2 = perf_counter()\n",
    "        diff = (t2 - t1) * 1000 # in ms\n",
    "        print(f\"Time elapsed: {diff:.2f} ms\")\n",
    "        return out\n",
    "    return timer\n",
    "\n",
    "def cdiv(a: int, b: int): return (a + b - 1) // b # equivalent to math.ceil()\n",
    "\n",
    "@dataclass\n",
    "class D3:\n",
    "    z: int = 1\n",
    "    y: int = 1\n",
    "    x: int = 1\n",
    "    @property\n",
    "    def size(self): return self.x * self.y * self.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecaff1e0-0059-4b01-ad3b-cac7ddccbed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_cuda_kernel(blocks: D3, threads: D3, malloc_shared_size: int=0, kernel: Callable=None):\n",
    "    assert kernel is not None\n",
    "    @timeit\n",
    "    def dispatch_kernel(*kernargs):\n",
    "        workers = blocks.size * threads.size\n",
    "        barrier = Barrier(workers)\n",
    "        with ThreadPoolExecutor(max_workers=workers) as e:\n",
    "            for bz, by, bx in product(range(blocks.z), range(blocks.y), range(blocks.x)):\n",
    "                shared_mem = torch.empty((malloc_shared_size,), dtype=DTYPE)\n",
    "                for tz, ty, tx in product(range(threads.z), range(threads.y), range(threads.x)):\n",
    "                    e.submit(kernel, D3(bz, by, bx), D3(tz, ty, tx), threads, barrier, shared_mem, *kernargs)\n",
    "    return dispatch_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddce8301-973a-478e-8c38-f54540e571b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test example\n",
    "M, K, N = 12, 24, 15\n",
    "A = torch.randn(M, K, dtype=DTYPE)\n",
    "B = torch.randn(K, N, dtype=DTYPE)\n",
    "C_ref = A @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bb7521e-5f28-4f9f-b013-e5bf5faa1821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(C: Tensor): return \"PASSED\" if torch.allclose(C, C_ref, atol=1e-6) else \"FAILED\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede18360-02b4-4ec2-b71d-3c3cf3314708",
   "metadata": {},
   "source": [
    "# Matmul Naive 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec45f063-215b-4712-b4fa-c2a03d40e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_naive_kernel(blockIdx: D3, threadIdx: D3, blockSize: D3, barrier: Barrier, shared_mem: Tensor,\n",
    "                        C: Tensor, A: Tensor, B: Tensor, M: int, K: int, N: int, counter: Counter):\n",
    "    idx = (blockIdx.x * blockSize.x) + threadIdx.x\n",
    "    if idx < M * N:\n",
    "        acc = 0.\n",
    "        m, n = idx // N, idx % N\n",
    "        for k in range(K):\n",
    "            acc += A[m * K + k] * B[k * N + n]\n",
    "            counter.inc_reads(2)\n",
    "        C[idx] = acc\n",
    "        counter.inc_writes(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64999740-393e-42e2-9f25-dd43845724f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_naive(A: Tensor, B: Tensor):\n",
    "    (M, K), (K_, N) = A.shape, B.shape\n",
    "    assert K == K_, f\"inner dims should match! {K} != {K_}\"\n",
    "    A = A.contiguous() if not A.is_contiguous() else A\n",
    "    B = B.contiguous() if not B.is_contiguous() else B\n",
    "    C = torch.empty(M, N, dtype=DTYPE)\n",
    "\n",
    "    threads = 8\n",
    "    blocks = cdiv(M * N, threads)\n",
    "    kernel = launch_cuda_kernel(D3(x=blocks), D3(x=threads), kernel=matmul_naive_kernel)\n",
    "    kernel(C.flatten(), A.flatten(), B.flatten(), M, K, N, counter:=Counter())\n",
    "    counter.show()\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12fdd2c7-3693-4bae-b4c6-8b08b56162b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 98.41 ms\n",
      "Total reads: 8640\n",
      "Total writes: 180\n",
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "C = matmul_naive(A, B)\n",
    "print(check(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8989d087-f875-4ac7-a48a-21076ce0b1c6",
   "metadata": {},
   "source": [
    "# Matmul Naive 2D Tiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dc1b533-64ab-4555-9c8c-0b643ab12114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_naive2D_kernel(blockIdx: D3, threadIdx: D3, blockSize: D3, barrier: Barrier, shared_mem: Tensor,\n",
    "                          C: Tensor, A: Tensor, B: Tensor, M: int, K: int, N: int, tileWidth: int, counter: Counter):\n",
    "    idx_m = (blockIdx.y * blockSize.y) + threadIdx.y\n",
    "    idx_n = (blockIdx.x * blockSize.x) + threadIdx.x\n",
    "    tiles_k = cdiv(K, tileWidth)\n",
    "    if idx_m < M and idx_n < N:\n",
    "        acc = 0.\n",
    "        for tile_k in range(tiles_k):\n",
    "            for k in range(tileWidth):\n",
    "                idx_k = tile_k * tileWidth + k\n",
    "                if idx_k < K:\n",
    "                    acc += A[idx_m * K + idx_k] * B[idx_k * N + idx_n]\n",
    "                    counter.inc_reads(2)\n",
    "        C[idx_m * N + idx_n] += acc\n",
    "        counter.inc_writes(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5d020a3-dbd3-456d-8d53-48e067426a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_naive2D(A: Tensor, B: Tensor, tileWidth: int):\n",
    "    (M, K), (K_, N) = A.shape, B.shape\n",
    "    assert K == K_, f\"inner dims should match! {K} != {K_}\"\n",
    "    A = A.contiguous() if not A.is_contiguous() else A\n",
    "    B = B.contiguous() if not B.is_contiguous() else B\n",
    "    C = torch.empty(M, N, dtype=DTYPE)\n",
    "\n",
    "    blocks_m = cdiv(M, tileWidth)\n",
    "    blocks_n = cdiv(N, tileWidth)\n",
    "    kernel = launch_cuda_kernel(\n",
    "        D3(y=blocks_m, x=blocks_n), D3(y=tileWidth, x=tileWidth), kernel=matmul_naive2D_kernel)\n",
    "    kernel(C.flatten(), A.flatten(), B.flatten(), M, K, N, tileWidth, counter:=Counter())\n",
    "    counter.show()\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81165d62-5374-4a4d-8f7a-4a23193aed58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 117.39 ms\n",
      "Total reads: 8640\n",
      "Total writes: 180\n",
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "C = matmul_naive2D(A, B, 5)\n",
    "print(check(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad847a-b3c6-4bc8-879a-89ed5a59713f",
   "metadata": {},
   "source": [
    "# Matmul 2D Tiled (Shared Memory Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c95319fe-3423-4cfe-b4f3-786e397291d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_tiled2D_kernel(blockIdx: D3, threadIdx: D3, blockSize: D3, barrier: Barrier, shared_mem: Tensor,\n",
    "                          C: Tensor, A: Tensor, B: Tensor, M: int, K: int, N: int, tileWidth: int, counter: Counter):\n",
    "    idx_m = (blockIdx.y * blockSize.y) + threadIdx.y\n",
    "    idx_n = (blockIdx.x * blockSize.x) + threadIdx.x\n",
    "    shared_offset = tileWidth ** 2\n",
    "    tiles_k = cdiv(K, tileWidth) \n",
    "    if idx_m < M and idx_n < N:\n",
    "        acc = 0.\n",
    "        for tile_k in range(tiles_k):\n",
    "            # load tile onto shared memory\n",
    "            idx_ak = tile_k * tileWidth + threadIdx.x\n",
    "            idx_bk = tile_k * tileWidth + threadIdx.y\n",
    "            shared_mem[threadIdx.y * tileWidth + threadIdx.x] = A[idx_m * K + idx_ak] if idx_ak < K else 0\n",
    "            shared_mem[threadIdx.y * tileWidth + threadIdx.x + shared_offset] = B[idx_bk * N + idx_n] if idx_bk < K else 0\n",
    "            if idx_ak < K: counter.inc_reads(1)\n",
    "            if idx_bk < K: counter.inc_reads(1)\n",
    "            barrier.wait()\n",
    "            # compute dot products on tile\n",
    "            for k in range(tileWidth):\n",
    "                idx_k = tile_k * tileWidth + k\n",
    "                if idx_k < K:\n",
    "                    acc += shared_mem[threadIdx.y * tileWidth + k] * shared_mem[shared_offset + k * tileWidth + threadIdx.x]\n",
    "            barrier.wait()\n",
    "        C[idx_m * N + idx_n] += acc\n",
    "        counter.inc_writes(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "117991f0-fdf4-432c-b3da-8f060063a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_tiled2D(A: Tensor, B: Tensor, tileWidth: int):\n",
    "    (M, K), (K_, N) = A.shape, B.shape\n",
    "    assert K == K_, f\"inner dims should match! {K} != {K_}\"\n",
    "    A = A.contiguous() if not A.is_contiguous() else A\n",
    "    B = B.contiguous() if not B.is_contiguous() else B\n",
    "    C = torch.empty(M, N, dtype=DTYPE)\n",
    "\n",
    "    blocks_m = cdiv(M, tileWidth)\n",
    "    blocks_n = cdiv(N, tileWidth)\n",
    "    kernel = launch_cuda_kernel(\n",
    "        D3(y=blocks_m, x=blocks_n), D3(y=tileWidth, x=tileWidth), 2*tileWidth**2, kernel=matmul_tiled2D_kernel)\n",
    "    kernel(C.flatten(), A.flatten(), B.flatten(), M, K, N, tileWidth, counter:=Counter())\n",
    "    counter.show()\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09f30530-4b18-4360-b6c8-52b81ff21c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 164.31 ms\n",
      "Total reads: 2880\n",
      "Total writes: 180\n",
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "C = matmul_tiled2D(A, B, 3)\n",
    "print(check(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a935917-0870-4c1f-a2d3-e9b43e73b907",
   "metadata": {},
   "source": [
    "# Matmul Naive 3D Tiled (for fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b763202-7a5a-49ac-a827-1849792770fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_naive3D_kernel(blockIdx: D3, threadIdx: D3, blockSize: D3, barrier: Barrier, shared_mem: Tensor,\n",
    "                          C: Tensor, A: Tensor, B: Tensor, M: int, K: int, N: int, counter: Counter):\n",
    "    idx_m = (blockIdx.z * blockSize.z) + threadIdx.z\n",
    "    idx_n = (blockIdx.y * blockSize.y) + threadIdx.y\n",
    "    idx_k = (blockIdx.x * blockSize.x) + threadIdx.x\n",
    "    if idx_m < M and idx_n < N and idx_k < K:\n",
    "        C[idx_m * N + idx_n] += A[idx_m * K + idx_k] * B[idx_k * N + idx_n]\n",
    "        counter.inc_reads(2)\n",
    "        counter.inc_writes(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21c9a76e-eebf-4e43-83bf-99902c2c6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_naive3D(A: Tensor, B: Tensor, tileWidth: int):\n",
    "    (M, K), (K_, N) = A.shape, B.shape\n",
    "    assert K == K_, f\"inner dims should match! {K} != {K_}\"\n",
    "    A = A.contiguous() if not A.is_contiguous() else A\n",
    "    B = B.contiguous() if not B.is_contiguous() else B\n",
    "    C = torch.empty(M, N, dtype=DTYPE)\n",
    "\n",
    "    blocks_m, blocks_n, blocks_k = cdiv(M, tileWidth), cdiv(N, tileWidth), cdiv(K, tileWidth)\n",
    "    kernel = launch_cuda_kernel(\n",
    "        D3(z=blocks_m, y=blocks_n, x=blocks_k), D3(tileWidth, tileWidth, tileWidth), kernel=matmul_naive3D_kernel)\n",
    "    kernel(C.flatten(), A.flatten(), B.flatten(), M, K, N, counter:=Counter())\n",
    "    counter.show()\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f914e7a-f68f-4122-979b-041320508b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 291.22 ms\n",
      "Total reads: 8640\n",
      "Total writes: 4320\n",
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "C = matmul_naive3D(A, B, 3)\n",
    "print(check(C))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
